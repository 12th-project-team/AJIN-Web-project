import streamlit as st
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from pathlib import Path
import os
from dotenv import load_dotenv

# --- í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ---
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    st.error("âŒ OpenAI API í‚¤ê°€ í™˜ê²½ë³€ìˆ˜ì— ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# --- ê²½ë¡œ ì„¤ì • ---
pdf_path = Path(r"C:\woohyun\AJIN-12th-project\data\ì»´í™œ1ê¸‰.pdf")
save_dir = Path.cwd() / "faiss_db"
save_dir.mkdir(parents=True, exist_ok=True)

# --- ë²¡í„°ìŠ¤í† ì–´ ìƒì„± í•¨ìˆ˜ ---
@st.cache_data(show_spinner=True)
def create_vectorstore():
    if not pdf_path.exists():
        st.error(f"PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        st.stop()

    loader = PyPDFLoader(str(pdf_path))
    all_docs = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    split_docs = text_splitter.split_documents(all_docs)

    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    vectorstore = FAISS.from_documents(split_docs, embeddings)
    vectorstore.save_local(folder_path=str(save_dir), index_name="index")
    return vectorstore

# --- ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ í•¨ìˆ˜ ---
@st.cache_resource(show_spinner=True)
def load_vectorstore():
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    vectorstore = FAISS.load_local(
        str(save_dir),
        embeddings,
        index_name="index",
        allow_dangerous_deserialization=True
    )
    return vectorstore

# --- ë©”ì¸ ---
st.title("ğŸ“š ì»´í™œ1ê¸‰ PDF ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì±—ë´‡")

# ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ë¯¸ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì˜¤ê¸°, ì—†ìœ¼ë©´ ìƒì„±
if not (save_dir / "index.faiss").exists():
    st.info("ë²¡í„°ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. PDFë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
    vectorstore = create_vectorstore()
    st.success("ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ!")
else:
    vectorstore = load_vectorstore()

retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})

prompt_template = """
ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ìœ ëŠ¥í•œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì°¸ê³  ë¬¸ì„œì…ë‹ˆë‹¤:

{context}

ì§ˆë¬¸: {question}

ì°¸ê³  ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µí•˜ì„¸ìš”. ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
"""
prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

llm = ChatOpenAI(model_name="gpt-4o", temperature=0, openai_api_key=openai_api_key)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs={"prompt": prompt}
)

query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

if query:
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        response = qa_chain.invoke({"query": query})
    st.write("ğŸ” ì§ˆë¬¸:", query)
    st.write("ğŸ’¬ ë‹µë³€:", response['result'])
