import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from pathlib import Path
import os
from dotenv import load_dotenv
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

# nltk punkt tokenizer ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ í•„ìš”)
nltk.download('punkt')

# --- í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ---
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    st.error("âŒ OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# --- ê²½ë¡œ ì„¤ì • ---
pdf_path = Path(r"C:\woohyun\AJIN-12th-project\data\ì»´í™œ1ê¸‰.pdf")
save_dir = Path.cwd() / "faiss_db"
save_dir.mkdir(parents=True, exist_ok=True)

# --- ë²¡í„°ìŠ¤í† ì–´ ìƒì„± í•¨ìˆ˜ ---
@st.cache_data(show_spinner=True)
def create_vectorstore():
    if not pdf_path.exists():
        st.error(f"PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        st.stop()

    loader = PyPDFLoader(str(pdf_path))
    all_docs = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    split_docs = text_splitter.split_documents(all_docs)

    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    vectorstore = FAISS.from_documents(split_docs, embeddings)
    vectorstore.save_local(folder_path=str(save_dir), index_name="index")
    return vectorstore

# --- ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ í•¨ìˆ˜ ---
@st.cache_resource(show_spinner=True)
def load_vectorstore():
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    vectorstore = FAISS.load_local(
        str(save_dir),
        embeddings,
        index_name="index",
        allow_dangerous_deserialization=True
    )
    return vectorstore

# --- ë²¡í„°ìŠ¤í† ì–´ ì¤€ë¹„ ---
st.title("ğŸ§  ì»´í™œ1ê¸‰ PDF ê¸°ë°˜ GPT vs RAG ì„±ëŠ¥ ë¹„êµ")

if not (save_dir / "index.faiss").exists():
    st.info("ğŸ”„ ë²¡í„°ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. PDFë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
    vectorstore = create_vectorstore()
    st.success("âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ!")
else:
    vectorstore = load_vectorstore()

retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})

# GPT ë‹¨ë… ëª¨ë¸
llm_gpt = ChatOpenAI(model_name="gpt-4o", temperature=0.3, openai_api_key=openai_api_key)

# RAG (ë²¡í„°ê²€ìƒ‰ + LLM)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm_gpt,
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs={"prompt": PromptTemplate(
        template="""
ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ìœ ëŠ¥í•œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì°¸ê³  ë¬¸ì„œì…ë‹ˆë‹¤:

{context}

ì§ˆë¬¸: {question}

ì°¸ê³  ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µí•˜ì„¸ìš”. ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
""",
        input_variables=["context", "question"]
    )}
)

# --- ì‚¬ìš©ì ì…ë ¥ UI ---
query = st.text_input("â“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
ground_truth = st.text_area("ğŸ” ì •ë‹µ(ê¸°ëŒ€ ë‹µë³€)ì„ ì…ë ¥í•˜ì„¸ìš”:")

if st.button("ë¹„êµ ì‹¤í–‰") and query and ground_truth:
    # GPT ë‹¨ë… ë‹µë³€ ìƒì„±
    with st.spinner("GPT ë‹¨ë… ë‹µë³€ ìƒì„± ì¤‘..."):
        gpt_response = llm_gpt.invoke(f"ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì¤˜:\n{query}")
        gpt_answer = gpt_response.content.strip()
    st.subheader("ğŸ¤– GPT ë‹¨ë… ë‹µë³€")
    st.write(gpt_answer)

    # RAG ë‹µë³€ ìƒì„±
    with st.spinner("RAG ë‹µë³€ ìƒì„± ì¤‘..."):
        rag_response = qa_chain.invoke({"query": query})
        rag_answer = rag_response["result"].strip()
    st.subheader("ğŸ§  RAG ë‹µë³€")
    st.write(rag_answer)

    # ì •ë‹µ í† í°í™” (BLEU ê³„ì‚° ì¤€ë¹„)
    reference = [nltk.word_tokenize(ground_truth.lower())]
    candidate_gpt = nltk.word_tokenize(gpt_answer.lower())
    candidate_rag = nltk.word_tokenize(rag_answer.lower())

    # BLEU ê³„ì‚° (smooth method ì‚¬ìš©)
    smoothie = SmoothingFunction().method4
    bleu_gpt = sentence_bleu(reference, candidate_gpt, smoothing_function=smoothie)
    bleu_rag = sentence_bleu(reference, candidate_rag, smoothing_function=smoothie)

    st.subheader("ğŸ“Š ì •ëŸ‰ í‰ê°€ (BLEU ì ìˆ˜)")
    st.write(f"GPT ë‹¨ë… BLEU ì ìˆ˜: {bleu_gpt:.4f}")
    st.write(f"RAG BLEU ì ìˆ˜: {bleu_rag:.4f}")

    # ê°„ë‹¨ ì •í™•ë„ í‰ê°€ (ë‹µë³€ì´ ì •ë‹µ í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€)
    acc_gpt = int(ground_truth.strip() in gpt_answer)
    acc_rag = int(ground_truth.strip() in rag_answer)

    st.subheader("ğŸ“Š ê°„ë‹¨ ì •í™•ë„ í‰ê°€ (ì •ë‹µ í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€)")
    st.write(f"GPT ë‹¨ë… ì •í™•ë„: {acc_gpt}")
    st.write(f"RAG ì •í™•ë„: {acc_rag}")
else:
    st.info("ì§ˆë¬¸ê³¼ ì •ë‹µì„ ëª¨ë‘ ì…ë ¥í•œ í›„ 'ë¹„êµ ì‹¤í–‰' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
