import streamlit as st
from langchain_openai import ChatOpenAI
from vectorstore_utils import load_chroma_vectorstore
import os

CATEGORY_NAME = "ì»´í“¨í„°í™œìš©ëŠ¥ë ¥"

def render():
    st.header(f"ğŸ¤– {CATEGORY_NAME} - ë¬¸ì„œ ê¸°ë°˜ ì±—ë´‡")

    base_path = os.path.join("chroma_db", CATEGORY_NAME)
    if not os.path.exists(base_path):
        st.info("â— ì €ì¥ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        return

    subfolders = os.listdir(base_path)
    if not subfolders:
        st.info("â— ì €ì¥ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        return

    selected_doc = st.selectbox("ì§ˆë¬¸í•  ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”", subfolders)

    try:
        vectordb = load_chroma_vectorstore(CATEGORY_NAME, selected_doc)
    except Exception as e:
        st.error(f"ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    retriever = vectordb.as_retriever(search_kwargs={"k": 3})
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

    query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

    def get_context(query):
        docs = retriever.get_relevant_documents(query)
        return "\n".join([doc.page_content for doc in docs])

    if st.button("ë‹µë³€ ìƒì„±") and query:
        context = get_context(query)
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            prompt = f"""
ì•„ë˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

[ì§ˆë¬¸]
{query}

[ë¬¸ì„œ ë‚´ìš©]
{context}
"""
            response = llm.invoke(prompt)

        st.subheader("ğŸ¤– ë‹µë³€")
        if hasattr(response, "content"):
            st.write(response.content)
        else:
            st.write(response)  # fallback for str or ê¸°íƒ€ ê°ì²´
