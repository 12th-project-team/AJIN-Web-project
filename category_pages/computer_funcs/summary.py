import streamlit as st
from langchain_openai import ChatOpenAI
from vectorstore_utils import load_chroma_vectorstore
import os

CATEGORY_NAME = "ì»´í“¨í„°í™œìš©ëŠ¥ë ¥"

def render():
    st.header(f"ğŸ“Œ {CATEGORY_NAME} - ìš”ì ì •ë¦¬")

    base_path = os.path.join("chroma_db", CATEGORY_NAME)
    if not os.path.exists(base_path):
        st.info("â— ì €ì¥ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        return

    subfolders = os.listdir(base_path)
    if not subfolders:
        st.info("â— ì €ì¥ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        return

    selected_doc = st.selectbox("ìš”ì•½í•  ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”", subfolders)

    try:
        vectordb = load_chroma_vectorstore(CATEGORY_NAME, selected_doc)
    except Exception as e:
        st.error(f"ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    retriever = vectordb.as_retriever(search_kwargs={"k": 20})  # ë‹¤ì–‘í•œ ë§¥ë½ í™•ë³´
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.1)

    query = st.text_input(
        "ìš”ì•½í•  ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        placeholder="ì˜ˆ) í•¨ìˆ˜ ì¢…ë¥˜ì™€ íŠ¹ì§•, ì—‘ì…€ ë…¼ë¦¬ì—°ì‚°ì, ìš´ì˜ì²´ì œì˜ ì—­í•  ë“±"
    )

    if st.button("ìš”ì•½ ì‹¤í–‰"):
        if not query.strip():
            st.warning("ìš”ì•½í•  ì£¼ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return

        docs = retriever.get_relevant_documents(query)
        # ì£¼ì œ í‚¤ì›Œë“œ filtering (korean/english ë„ì–´ì“°ê¸°, ì½¤ë§ˆ êµ¬ë¶„)
        keywords = [kw.strip() for kw in query.replace(',', ' ').split()]
        filtered_docs = [
            doc for doc in docs if any(kw in doc.page_content for kw in keywords)
        ]
        if not filtered_docs:
            st.info("â— í•´ë‹¹ ì£¼ì œì™€ ê´€ë ¨ëœ ë‚´ìš©ì´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        context = "\n".join([doc.page_content for doc in filtered_docs if doc.page_content.strip()])

        prompt = f"""
ë„ˆëŠ” ì»´í“¨í„°í™œìš©ëŠ¥ë ¥ ê³µì‹ ë¬¸ì„œ ê¸°ë°˜ ìš”ì•½ ì „ë¬¸ê°€ì´ì, ì±„ì ê´€ì´ì•¼.

[í•„ìˆ˜ ì§€ì¹¨]
- ë°˜ë“œì‹œ ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì—ì„œ '{query}'ì™€ ì§ì ‘ì  ë˜ëŠ” ë¶€ë¶„ì ìœ¼ë¡œ ì—°ê´€ëœ ë‚´ìš©ë„ ëª¨ë‘ í¬í•¨ ë½‘ì•„ë¼.
- '{query}'ì™€ ê´€ë ¨ ì—†ëŠ” ë°°ê²½ ì„¤ëª…, ì£¼ë³€ ê°œë…, ì¶”ì¸¡, ì¼ë°˜ë¡ , ì‚¬ì¡± ë“±ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆë¼.
- ì£¼ì œì™€ ì™„ì „íˆ ë¬´ê´€í•œ ë¬¸ì¥ì€ ì ˆëŒ€ ìƒì„±í•˜ì§€ ë§ˆë¼. ê·¼ê±° ì—†ëŠ” ì¼ë°˜ë¡ , ìƒì‹, ë¶€ì—°ì„¤ëª… ê¸ˆì§€.
- 'ê´€ë ¨ ë‚´ìš© ì—†ìŒ'ì€ í•´ë‹¹ ì£¼ì œ í‚¤ì›Œë“œê°€ ì‹¤ì œ ë¬¸ì„œì— ì—†ì„ ë•Œë§Œ ì¶œë ¥í•˜ë¼.
- ì‹¤ì œ ì‹œí—˜ ëŒ€ë¹„ì— í•„ìš”í•œ **êµ¬ì²´ì , ì‹¤ë¬´ì , ëª…í™•í•œ ìš”ì•½**ë§Œ ì œì‹œí•˜ë¼.

[ì¶œë ¥ í˜•ì‹]
- ê´€ë ¨ í•µì‹¬ ë‚´ìš©ì„ 10ê°œ ì´ë‚´ì˜ ì†Œì£¼ì œë¡œ ë¶„ë¥˜í•˜ì—¬, ê° ì†Œì£¼ì œ(ì†Œë‹¨ë½)ë§ˆë‹¤ **í•œê¸€ ì œëª©**ì„ ë¶™ì´ê³ , ê° ë‹¨ë½ì„ 3~5ì¤„ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ë¼.
- ë°˜ë“œì‹œ ë²ˆí˜¸ì™€ ì œëª©, ìš”ì•½ì´ êµ¬ë¶„ë˜ê²Œ ì˜ˆì‹œì²˜ëŸ¼ ì¶œë ¥í•˜ë¼.

[ì¶œë ¥ ì˜ˆì‹œ]
1. [ì†Œì£¼ì œ1 ì œëª©]: (3~5ì¤„ ìš”ì•½)
2. [ì†Œì£¼ì œ2 ì œëª©]: (3~5ì¤„ ìš”ì•½)
...

[ë¬¸ì„œì—ì„œ '{query}'ì™€ ì§ì ‘ì ìœ¼ë¡œ ì—°ê´€ëœ ë¬¸ë‹¨(ë¬¸ì¥)ë§Œ í™œìš©í•˜ì—¬ ìš”ì•½í•˜ë¼.]

-------------------------
{context}
-------------------------
"""
        with st.spinner("ğŸ“˜ ìš”ì•½ ì¤‘..."):
            response = llm.invoke(prompt)
        st.subheader("ğŸ“˜ ìš”ì ì •ë¦¬ ê²°ê³¼")
        st.write(response.content if hasattr(response, "content") else response)
