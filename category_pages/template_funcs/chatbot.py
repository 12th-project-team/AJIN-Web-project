# category_pages/template_funcs/chatbot.py

import os
import streamlit as st
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

CHAT_STYLE = """<style>
.chat-container { max-width:700px; margin:0 auto; }
.bubble { padding:12px; border-radius:20px; margin:8px 0; max-width:80%; display:inline-block; }
.user { background:#DCF8C6; float:right; clear:both; }
.bot { background:#F1F0F0; float:left; clear:both; }
.clearfix::after { content:""; clear:both; display:table; }
</style>"""

custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸:
{question}

ë‹µë³€:
"""
)

def render(category_name: str):
    st.markdown(CHAT_STYLE, unsafe_allow_html=True)
    st.header(f"ğŸ¤– {category_name} - ì±—ë´‡")

    base = os.path.join("chroma_db", category_name)
    if not os.path.exists(base):
        st.info("â— ì €ì¥ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    docs = [
        d for d in os.listdir(base)
        if os.path.isdir(os.path.join(base, d))
    ]
    if not docs:
        st.info("â— ì €ì¥ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    doc = st.selectbox("ì§ˆë¬¸í•  ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”", docs, key=f"chat_doc_{category_name}")
    for q,a in st.session_state.chat_history:
        st.markdown(f'<div class="clearfix"><div class="bubble user">ğŸ‘¤ {q}</div></div>', unsafe_allow_html=True)
        st.markdown(f'<div class="clearfix"><div class="bubble bot">ğŸ¤– {a}</div></div>', unsafe_allow_html=True)

    question = st.text_input(
        "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
        key=f"chat_input_{category_name}",
        placeholder="Enter í‚¤ë¡œ ì „ì†¡",
        on_change=lambda: _on_submit(category_name)
    )

def _on_submit(category_name: str):
    user_q = st.session_state[f"chat_input_{category_name}"]
    if not user_q.strip():
        return

    base = os.path.join("chroma_db", category_name, st.session_state[f"chat_doc_{category_name}"])
    store = Chroma(
        persist_directory=base,
        embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    )
    retriever = store.as_retriever(search_type="similarity", k=3)
    llm = ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo", openai_api_key=OPENAI_API_KEY)
    qa = RetrievalQA.from_chain_type(
        llm=llm, chain_type="stuff", retriever=retriever,
        chain_type_kwargs={"prompt": custom_prompt}
    )
    answer = qa.run(user_q)
    st.session_state.chat_history.append((user_q, answer))
    st.session_state[f"chat_input_{category_name}"] = ""
