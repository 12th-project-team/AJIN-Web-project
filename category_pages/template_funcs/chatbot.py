import os
import streamlit as st
from langchain_community.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

# ê°„ë‹¨í•œ ë§í’ì„  ìŠ¤íƒ€ì¼
CHAT_CSS = """
<style>
.bubble { padding:8px 12px; border-radius:12px; margin:4px 0; }
.user { background:#DCF8C6; text-align:right; }
.bot  { background:#F1F0F0; text-align:left; }
</style>
"""

# ì»¤ìŠ¤í…€ QA í”„ë¡¬í”„íŠ¸
PROMPT = PromptTemplate(
    input_variables=["context", "question"],
    template="""
ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.

ë¬¸ì„œ:
{context}

ì§ˆë¬¸:
{question}

ë‹µë³€:
"""
)

def render(category_name: str):
    st.markdown(CHAT_CSS, unsafe_allow_html=True)
    st.header(f"ğŸ’¬ {category_name} - ë¬¸ì„œ ê¸°ë°˜ ì±—ë´‡")

    base_path = os.path.join("chroma_db", category_name)
    if not os.path.exists(base_path):
        st.info("â— ì €ì¥ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    docs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]
    if not docs:
        st.info("â— ì €ì¥ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ë¬¸ì„œ ì„ íƒ
    selected = st.selectbox("ì§ˆë¬¸í•  ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”", docs, key=f"chat_doc_{category_name}")

    # ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    # ê¸°ë¡ ë Œë”ë§
    for q,a in st.session_state.chat_history:
        st.markdown(f"<div class='bubble user'>ğŸ‘¤ {q}</div>", unsafe_allow_html=True)
        st.markdown(f"<div class='bubble bot'>ğŸ¤– {a}</div>", unsafe_allow_html=True)

    # ì§ˆë¬¸ ì…ë ¥
    question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", key=f"chat_input_{category_name}")
    if st.button("ì „ì†¡", key=f"chat_send_{category_name}") and question:
        try:
            # ë²¡í„° ë¶ˆëŸ¬ì˜¤ê¸°
            vectordb = Chroma(
                persist_directory=os.path.join(base_path, selected),
                embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
            )
            retriever = vectordb.as_retriever(search_type="similarity", k=3)
            llm = ChatOpenAI(temperature=0, model_name="gpt-4o-mini", openai_api_key=OPENAI_API_KEY)
            chain = RetrievalQA.from_chain_type(
                llm=llm, chain_type="stuff", retriever=retriever, chain_type_kwargs={"prompt": PROMPT}
            )
            answer = chain.run(question)
            st.session_state.chat_history.append((question, answer))
        except Exception as e:
            st.error(f"ì±—ë´‡ ì˜¤ë¥˜: {e}")
