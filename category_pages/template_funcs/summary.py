import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from vectorstore_utils import load_chroma_vectorstore
import os

def render(category_name: str):
    st.header(f"ğŸ“Œ {category_name} - ìš”ì•½")

    base_path = os.path.join("chroma_db", category_name)
    if not os.path.exists(base_path):
        st.info("â— ì €ì¥ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        return

    subfolders = os.listdir(base_path)
    if not subfolders:
        st.info("â— ì €ì¥ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        return

    selected_doc = st.selectbox("ìš”ì•½í•  ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”", subfolders)

    try:
        vectordb = load_chroma_vectorstore(category_name, selected_doc)
    except Exception as e:
        st.error(f"ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    retriever = vectordb.as_retriever(search_kwargs={"k": 3})
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

    query = st.text_input("ìš”ì•½í•  ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ) í•µì‹¬ ë‚´ìš©, ì£¼ìš” í‚¤ì›Œë“œ ë“±")

    if st.button("ìš”ì•½ ì‹¤í–‰"):
        if not query.strip():
            st.warning("ìš”ì•½í•  ì£¼ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return

        docs = retriever.get_relevant_documents(query)
        context = "\n".join([doc.page_content for doc in docs])

        prompt = f"""
ì£¼ì œ 10ê°œ ì •ë„ë¡œ ë‚˜ëˆ ì„œ ë‹¤ìŒ ë‚´ìš©ì„ ê°„ë‹¨íˆ 3~5ì¤„ì”© ìš”ì•½í•´ì¤˜.
ë‚´ìš©ì´ ì—†ìœ¼ë©´ 'ë‚´ìš© ì—†ìŒ'ì´ë¼ê³  í•´ì¤˜:

{context}
"""

        with st.spinner("ğŸ“˜ ìš”ì•½ ì¤‘..."):
            result = llm.invoke([HumanMessage(content=prompt)])

        st.subheader("ğŸ“˜ ìš”ì•½ ê²°ê³¼")
        st.write(result.content)
