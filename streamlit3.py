import streamlit as st
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from pathlib import Path
import os
from dotenv import load_dotenv

# --- í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ---
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    st.error("âŒ OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# --- ê²½ë¡œ ì„¤ì • ---
pdf_path = Path(r"C:\woohyun\AJIN-12th-project\data\ì»´í™œ1ê¸‰.pdf")
save_dir = Path.cwd() / "faiss_db"
save_dir.mkdir(parents=True, exist_ok=True)

# --- ë²¡í„°ìŠ¤í† ì–´ ìƒì„± í•¨ìˆ˜ ---
@st.cache_data(show_spinner=True)
def create_vectorstore():
    if not pdf_path.exists():
        st.error(f"PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        st.stop()

    loader = PyPDFLoader(str(pdf_path))
    all_docs = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    split_docs = text_splitter.split_documents(all_docs)

    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    vectorstore = FAISS.from_documents(split_docs, embeddings)
    vectorstore.save_local(folder_path=str(save_dir), index_name="index")
    return vectorstore

# --- ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ í•¨ìˆ˜ ---
@st.cache_resource(show_spinner=True)
def load_vectorstore():
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    vectorstore = FAISS.load_local(
        str(save_dir),
        embeddings,
        index_name="index",
        allow_dangerous_deserialization=True
    )
    return vectorstore

# --- ë²¡í„°ìŠ¤í† ì–´ ì¤€ë¹„ ---
st.title("ğŸ§  ì»´í™œ1ê¸‰ PDF ê¸°ë°˜ í•™ìŠµ ë„ìš°ë¯¸")

if not (save_dir / "index.faiss").exists():
    st.info("ğŸ”„ ë²¡í„°ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. PDFë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
    vectorstore = create_vectorstore()
    st.success("âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ!")
else:
    vectorstore = load_vectorstore()

retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})
llm = ChatOpenAI(model_name="gpt-4o", temperature=0.3, openai_api_key=openai_api_key)

# --- ê³µí†µ í”„ë¡¬í”„íŠ¸ ---
base_prompt = PromptTemplate(
    template="""
ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ìœ ëŠ¥í•œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì°¸ê³  ë¬¸ì„œì…ë‹ˆë‹¤:

{context}

ì§ˆë¬¸: {question}

ì°¸ê³  ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µí•˜ì„¸ìš”. ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
""",
    input_variables=["context", "question"]
)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs={"prompt": base_prompt}
)

# --- ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ ---
query = st.text_input("â“ ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”:")

# --- ë²„íŠ¼ UI ---
col1, col2, col3, col4 = st.columns(4)

with col1:
    run_qa = st.button("ì§ˆë¬¸ ë‹µë³€")
with col2:
    run_summary = st.button("ìš”ì•½ ìš”ì²­")
with col3:
    run_quiz = st.button("í€´ì¦ˆ ìƒì„±")
with col4:
    run_exam = st.button("ê¸°ì¶œë¬¸ì œ ìƒì„±")

# --- ê´€ë ¨ ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸° ---
def get_context(query):
    docs = retriever.get_relevant_documents(query)
    return "\n".join([doc.page_content for doc in docs])

# --- ê¸°ëŠ¥ë³„ ì‹¤í–‰ ---
if query:
    context = get_context(query)

    if run_qa:
        with st.spinner("ğŸ§  ë‹µë³€ ìƒì„± ì¤‘..."):
            response = qa_chain.invoke({"query": query})
        st.subheader("ğŸ’¬ ë‹µë³€")
        st.write(response["result"])

    if run_summary:
        with st.spinner("ğŸ“˜ ìš”ì•½ ì¤‘..."):
            prompt = f"""
ì£¼ì œ 10ê°œì •ë„ ë‚˜ëˆ ì„œ ë‹¤ìŒ ë‚´ìš©ì„ ê°„ë‹¨íˆ 3~5ì¤„ë¡œ ìš”ì•½í•´ì¤˜:

{context}
"""
            result = llm.invoke(prompt)
        st.subheader("ğŸ“˜ ìš”ì•½ ê²°ê³¼")
        st.write(result.content)

    if run_quiz:
        with st.spinner("ğŸ“ í€´ì¦ˆ ë¬¸ì œ ìƒì„± ì¤‘..."):
            prompt = f"""
ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê°ê´€ì‹ í€´ì¦ˆ 10ê°œë¥¼ ë§Œë“¤ì–´ì¤˜. ê° ë¬¸ì œëŠ” ë³´ê¸° 4ê°œì™€ ì •ë‹µê³¼ êµ¬ì²´ì ì¸ í•´ì„¤ì„ í¬í•¨í•´ì¤˜.

{context}
"""
            result = llm.invoke(prompt)
        st.subheader("ğŸ“ í€´ì¦ˆ ë¬¸ì œ")
        st.write(result.content)

    if run_exam:
        with st.spinner("ğŸ“„ ê¸°ì¶œë¬¸ì œ ìŠ¤íƒ€ì¼ ìƒì„± ì¤‘..."):
            prompt = f"""
ë‹¤ìŒ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì»´í™œ 1ê¸‰ í•„ê¸° ì‹œí—˜ì—ì„œ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ê°ê´€ì‹ ë¬¸ì œ 10ê°œë¥¼ ë§Œë“¤ì–´ì¤˜.
ê° ë¬¸ì œëŠ” ë³´ê¸°ì™€ ì •ë‹µê³¼ êµ¬ì²´ì ì¸ í•´ì„¤ì„ í¬í•¨í•´.

{context}
"""
            result = llm.invoke(prompt)
        st.subheader("ğŸ“„ ê¸°ì¶œë¬¸ì œ")
        st.write(result.content)
